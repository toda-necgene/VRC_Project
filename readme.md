# VRC(Voice Realtime Conversion)

## 概要
それぞれ10分ぐらいの同一内容ではないない音声を学習し、

話者Aの声質を話者Bの声質に変換できます。

GTX1070なら2時間半ぐらいで完了します。

## インストール（環境構築）

**pythonのバージョンは3.6でお願いします**

以下のライブラリをインストールしてください。

- pyaudio(変換時のみ)
- pyworld
- numpy
- matplotlib(学習時のみ)
- chainer(cupyやchainerX等のGPU関係を利用すること推奨)

## 学習の手順

1. 10分ぐらい録音して保存, 配置
2. GPUを用いて学習
3. CPUで実行(リアルタイム変換)

### 1. 録音して保存、配置

"dataset"ディレクトリを用意して、以下に"train","test","patch"を用意してください。


#### trainフォルダ

学習用の音声が入ります。


学習を正常に収束させるために10分以上の録音を行ってください。
同じ文章を読まなくても構いません。
録音環境は**44100Hz以上、16bit以上**でお願いします。

(台本.txtを推奨します。読みなれている場合は２回ほど読んでも構わないです。噛んだり笑ったりした部分も削除する必要はありません)

録音が終わったら、前処理として２秒以上の無音区間を切り詰めてください。

**44100Hz,int16,モノラル,非圧縮音声ファイル(wavファイル)**の形式で
"dataset/train/A/"と"dataset/train/B/"に話者ごとに分けて保存してください。

**ただし、データが偏った場合は正常に学習できない場合があります。**


#### testフォルダ
同条件で10秒程度のテスト用ファイル(話者A)を用意して、
"test.wav"を"dataset/test"ディレクトリ以下に保存してください。

``` bash
 |
 |_dataset___train___A__変換元録音ファイル（10分程度）
           |       |_B__変換先録音ファイル（10分程度）
           |_test____test.wav（テスト用音声10秒程度）
           |
           |_patch
```


``` bash
 python train.py
```

### 学習
"setting.json"ファイルで学習の設定が行なえます。
主に出力先の設定を記述します。
以下の手順で音声ファイルを用意します。

学習は
`python train.py`
で、実行します。

## 関連リポジトリ

[become-yukarin](https://github.com/Hiroshiba/become-yukarin)

先駆者様
こちらは、パラレルデータを使用しており高性能

[World](https://github.com/mmorise/World)

音声合成・分析で使用させていただいております

[BURI様のqiita記事](https://qiita.com/BURI55/items/e9fdf381087b363c1074)

リポジトリではありませんが、関連情報はここにまとまっています

## ライセンス

MIT
