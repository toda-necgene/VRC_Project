# 学習用パラメータ探索レポート
## 探索したパラメータについて

 学習の安定度と性能の向上に着目し、生成器(以下G)のレイヤー数、及びチャンネル数を探索した。
 行った実験は以下の通りである。

 - モデルについて安定性を実験した

## 実験

### 条件
 各学習について800itrを50回ずつテストを行った。

 扱ったのは以下モデル
 - resnet(conv(k11c256)-leakyrelu-add)9blocks
 - resnet(conv(k11c128)-leakyrelu-add)9blocks
 - resnet(conv(k11c128)-leakyrelu-add)6blocks
 - resnet(depthwise_conv(k11c128)-leakyrelu-add-conv(k1c128)-leakyrelu-add)6blocks
 - resnet(depthwise_conv(k11c128)-leakyrelu-add-conv(k1c128)-leakyrelu-add)-9blocks
### 結果
![散布図](layer.png "scatter")
![箱ひげ図](hist_box.png "hist-and-box")
 
詳しいデータは[log.csv参照](log.csv)

### 考察

  以上の結果からレイヤー数9-256chまたはレイヤー数9-128chがモデルとしてよいことがわかる。
  
  レイヤー数9-256chはレイヤー数9-128chに比べ最良値はより優秀になっているが、結果の分散がやや大きい点が見受けられる。箱ひげ図を見た場合においても値が2極化している点も気になる。
  
  以上の点より可能性として、Gが優位になり一部で学習が崩壊していること,過学習を引き起こしていることが考えられる。
  
  対策として、前者は識別機（D）の性能を向上させることで解決する可能性がある。後者は128chのモデルが良いという結論に至る。
  
  また、短期的な評価のため学習を進め続けた場合の推移等は測定していない。そういった点も評価に含めて実験を進めていくことが今後の方針となった。
